import asyncio
import os
import logging
import re
from datetime import datetime
from typing import List, Dict, Tuple, Any

# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è Telegram
from aiogram import Bot, Dispatcher, types, F
from aiogram.filters import Command
from aiogram.fsm.storage.memory import MemoryStorage
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup

# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
import docx
import PyPDF2

# ============================================================
#  AGENT 1: PERCEPTION AGENT (–£–õ–£–ß–®–ï–ù–ù–´–ô PDF –ü–ê–†–°–ò–ù–ì)
# ============================================================
class PerceptionAgent:
    @staticmethod
    def _normalize_text(s: str) -> str:
        if s is None: return ""
        # 1. –ó–∞–º–µ–Ω–∞ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–π
        s = s.replace("_", " ").replace("\u00A0", " ").replace("\u200B", "").replace("\uFEFF", "")
        # 2. –ó–∞–º–µ–Ω–∞ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ –∏ —Ç–∞–±—É–ª—è—Ü–∏–π
        s = s.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
        # 3. FIX: –£–¥–∞–ª–µ–Ω–∏–µ "—Ä–∞–∑—Ä—ã–≤–æ–≤" —Å–ª–æ–≤ –≤ PDF (—Å–µ–Ω—Ç—è–± —Ä—è -> —Å–µ–Ω—Ç—è–±—Ä—è)
        # –ï—Å–ª–∏ –≤–∏–¥–∏–º "–±—É–∫–≤–∞-–ø—Ä–æ–±–µ–ª-–±—É–∫–≤–∞", –∏ —ç—Ç–æ –Ω–µ –ø—Ä–µ–¥–ª–æ–≥ (—ç–≤—Ä–∏—Å—Ç–∏–∫–∞)
        # (–≠—Ç–æ –±–∞–∑–æ–≤–∞—è –∑–∞—â–∏—Ç–∞, –∏–¥–µ–∞–ª—å–Ω–∞—è —Ç—Ä–µ–±—É–µ—Ç —Å–ª–æ–≤–∞—Ä—è)
        # –ó–¥–µ—Å—å –º—ã –ø—Ä–æ—Å—Ç–æ —Å—Ö–ª–æ–ø—ã–≤–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        s = re.sub(r"[ \t\v\f]+", " ", s)
        return s.strip()

    @staticmethod
    def load_content(path: str) -> List[str]:
        lower = path.lower()
        if lower.endswith(".docx"):
            return PerceptionAgent._load_docx(path)
        elif lower.endswith(".pdf"):
            return PerceptionAgent._load_pdf(path)
        else:
            raise ValueError("–§–æ—Ä–º–∞—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è")

    @staticmethod
    def _load_docx(path: str) -> List[str]:
        doc = docx.Document(path)
        paragraphs = []
        for p in doc.paragraphs:
            paragraphs.append(PerceptionAgent._normalize_text(p.text))
        for table in doc.tables:
            for row in table.rows:
                for cell in row.cells:
                    paragraphs.append(PerceptionAgent._normalize_text(cell.text))
        return paragraphs

    @staticmethod
    def _load_pdf(path: str) -> List[str]:
        lines = []
        with open(path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            for page in reader.pages:
                text = page.extract_text()
                if not text: continue
                
                # FIX: PyPDF2 —á–∞—Å—Ç–æ —Ä–∞–∑–±–∏–≤–∞–µ—Ç —Ç–∞–±–ª–∏—Ü—ã –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏.
                # –ú—ã –Ω–µ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Å—Ä–∞–∑—É, –∞ –æ—Ç–¥–∞–µ–º —Å—ã—Ä—ã–µ —Å—Ç—Ä–æ–∫–∏,
                # –Ω–æ SchemaAgent —Ç–µ–ø–µ—Ä—å —É–º–µ–µ—Ç –∏—Ö —Å–∫–ª–µ–∏–≤–∞—Ç—å.
                for ln in text.splitlines():
                    norm_ln = PerceptionAgent._normalize_text(ln)
                    if norm_ln:
                        lines.append(norm_ln)
        return lines

# ============================================================
#  AGENT 2: SCHEMA AGENT (FIX: –°–ö–õ–ï–ô–ö–ê –†–ê–ó–û–†–í–ê–ù–ù–´–• –¢–ï–ì–û–í)
# ============================================================
class SchemaAgent:
    def parse_template(self, paragraphs: List[str]) -> List[Dict]:
        placeholders = []
        # Regex —Ç–µ–ø–µ—Ä—å –¥–æ–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ–±–µ–ª—ã –≤–Ω—É—Ç—Ä–∏ —Ç–µ–≥–∞ –Ω–∞ —Å–ª—É—á–∞–π —Ä–∞–∑—Ä—ã–≤–æ–≤
        pattern = re.compile(
            r"\[\[\s*([^:\]\n]+?)\s*:\s*([^,:]\s*[^,\]\n]+?)"
            r"(?:\s*:\s*([^:\]\n]+?)\s*:\s*([^,\]\n]+?))?"
            r"(?:\s*,\s*(optional))?\s*\]\]",
            flags=re.IGNORECASE,
        )

        seen_names = set()
        
        # –ë–£–§–ï–† –î–õ–Ø –°–ö–õ–ï–ô–ö–ò –°–¢–†–û–ö
        # –ï—Å–ª–∏ —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç '[[', –Ω–æ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç ']]', –º—ã –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –µ—ë,
        # –∞ –∂–¥–µ–º —Å–ª–µ–¥—É—é—â—É—é, —á—Ç–æ–±—ã —Å–∫–ª–µ–∏—Ç—å. –≠—Ç–æ —á–∏–Ω–∏—Ç —Ç–∞–±–ª–∏—Ü—ã –≤ PDF.
        buffer = ""
        buffer_start_idx = 0

        processed_paragraphs = [] # –°–ø–∏—Å–æ–∫ (—Ç–µ–∫—Å—Ç, –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å)

        # 1. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: —Å–∫–ª–µ–π–∫–∞ —Ä–∞–∑–æ—Ä–≤–∞–Ω–Ω—ã—Ö —Ç–µ–≥–æ–≤
        for idx, para in enumerate(paragraphs):
            # –ï—Å–ª–∏ –≤ –±—É—Ñ–µ—Ä–µ —á—Ç–æ-—Ç–æ –µ—Å—Ç—å, –¥–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é —Å—Ç—Ä–æ–∫—É –∫ –Ω–µ–º—É
            if buffer:
                buffer += " " + para
                # –ï—Å–ª–∏ —Ç–µ–≥ –∑–∞–∫—Ä—ã–ª—Å—è
                if "]]" in para:
                    processed_paragraphs.append((buffer, buffer_start_idx))
                    buffer = ""
                continue
            
            # –ï—Å–ª–∏ –Ω–∞—á–∞–ª–æ —Ç–µ–≥–∞ –µ—Å—Ç—å, –∞ –∫–æ–Ω—Ü–∞ –Ω–µ—Ç ‚Äî –Ω–∞—á–∏–Ω–∞–µ–º –Ω–∞–∫–∞–ø–ª–∏–≤–∞—Ç—å –±—É—Ñ–µ—Ä
            if "[[" in para and "]]" not in para:
                buffer = para
                buffer_start_idx = idx
                continue
            
            # –û–±—ã—á–Ω–∞—è —Å—Ç—Ä–æ–∫–∞
            processed_paragraphs.append((para, idx))

        # 2. –ü–∞—Ä—Å–∏–Ω–≥
        for text, original_idx in processed_paragraphs:
            if not text.strip(): continue
            
            for m in pattern.finditer(text):
                name = m.group(1).strip()
                if name.lower() in seen_names: continue
                seen_names.add(name.lower())

                data = {
                    "name": name,
                    "type": self._normalize_type(m.group(2).strip()),
                    "group_name": m.group(3).strip() if m.group(3) else "",
                    "group_condition": m.group(4).strip() if m.group(4) else "",
                    "optional": bool(m.group(5)),
                    "anchor_before": "",
                }

                # –Ø–∫–æ—Ä—å –±–µ—Ä–µ–º –∏–∑ —Å–∫–ª–µ–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
                left_part = text[: m.start()].strip()
                if left_part:
                    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 —Å–∏–º–≤–æ–ª–æ–≤, —á—Ç–æ–±—ã —è–∫–æ—Ä—å –Ω–µ –±—ã–ª —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–º
                    data["anchor_before"] = left_part[-40:]
                else:
                    # –ò—â–µ–º –≤ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö "—á–∏—Å—Ç—ã—Ö" –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞—Ö
                    # (–£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è —Å–∫–ª–µ–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫)
                    stop_patterns = ["—É—Ç–≤–µ—Ä–∂–¥–∞—é", "–∑–∞–¥–∞–Ω–∏–µ", "–≤–≤–µ–¥–µ–Ω–∏–µ"]
                    for j in range(len(processed_paragraphs) - 1, -1, -1):
                        prev_txt, prev_idx = processed_paragraphs[j]
                        if prev_idx >= original_idx: continue # –ù–µ —Å–º–æ—Ç—Ä–∏–º –≤–ø–µ—Ä–µ–¥
                        
                        if prev_txt and "[[" not in prev_txt and not any(s in prev_txt.lower() for s in stop_patterns):
                            data["anchor_before"] = prev_txt
                            break

                placeholders.append(data)

        return placeholders

    def _normalize_type(self, t):
        t = t.lower()
        if t in ("int", "integer", "num", "number"): return "number"
        if t in ("str", "string", "text"): return "string"
        return t

# ============================================================
#  AGENT 3: EXTRACTION AGENT (–ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø)
# ============================================================
class ExtractionAgent:
    def __init__(self):
        self.stop_words = [
            "–≤–≤–µ–¥–µ–Ω–∏–µ", "–∑–∞–∫–ª—é—á–µ–Ω–∏–µ", "—Å–ø–∏—Å–æ–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤",
            "–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ", "–∑–∞–¥–∞–Ω–∏–µ", "—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å", "–∫—É—Ä–∞—Ç–æ—Ä", "–ø—Ä–æ–≤–µ—Ä—è—é—â–∏–π",
            "–Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —ç—Ç–∞–ø–æ–≤" # –î–æ–±–∞–≤–ª–µ–Ω–æ, —á—Ç–æ–±—ã –Ω–µ —Ö–≤–∞—Ç–∞–ª–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–∞–±–ª–∏—Ü
        ]

    def _clean_anchor(self, text):
        return re.sub(r'[^a-zA-Z–∞-—è–ê-–Ø0-9]', '', text).lower()

    def find_value(self, item: Dict, doc_paragraphs: List[str], start_cursor: int) -> Tuple[bool, str, int]:
        anchor = item.get("anchor_before")
        expected_type = item.get("type")
        
        if not anchor:
            return False, None, start_cursor

        clean_anchor = self._clean_anchor(anchor)
        
        # –ü–æ–∏—Å–∫ —Å—Ç—Ä–æ–∫–∏, —Å–æ–¥–µ—Ä–∂–∞—â–µ–π —è–∫–æ—Ä—å
        target_idx = -1
        for i in range(start_cursor, len(doc_paragraphs)):
            # –ù–µ—Å—Ç—Ä–æ–≥–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ: —Å–æ–¥–µ—Ä–∂–∏—Ç—Å—è –ª–∏ –æ—á–∏—â–µ–Ω–Ω—ã–π —è–∫–æ—Ä—å –≤ –æ—á–∏—â–µ–Ω–Ω–æ–π —Å—Ç—Ä–æ–∫–µ
            if clean_anchor in self._clean_anchor(doc_paragraphs[i]):
                target_idx = i
                break
        
        if target_idx == -1:
            return False, None, start_cursor

        # === –°–¢–†–ê–¢–ï–ì–ò–Ø 0: –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ò–ó –¢–û–ô –ñ–ï –°–¢–†–û–ö–ò (Same Line) ===
        # –≠—Ç–æ —Ä–µ—à–∏—Ç –ø—Ä–æ–±–ª–µ–º—É "3. –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ —Ñ—ã–≤" –∏ "–°–∞–ª—å–Ω–∏–∫–æ–≤"
        current_text = doc_paragraphs[target_idx]
        
        # –ù–∞—Ö–æ–¥–∏–º, –≥–¥–µ –∫–æ–Ω—á–∞–µ—Ç—Å—è —è–∫–æ—Ä—å –≤ —Ä–µ–∞–ª—å–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
        # (–£–ø—Ä–æ—â–µ–Ω–Ω–æ: —Ä–∞–∑–±–∏–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É –ø–æ —Ç–µ–∫—Å—Ç—É —è–∫–æ—Ä—è, –µ—Å–ª–∏ –æ–Ω —Ç–∞–º –µ—Å—Ç—å —Ü–µ–ª–∏–∫–æ–º)
        # –î–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º regex escape
        match = re.search(re.escape(PerceptionAgent._normalize_text(anchor)), current_text, re.IGNORECASE)
        
        candidate_same_line = ""
        if match:
            candidate_same_line = current_text[match.end():].strip()
        else:
            # –ï—Å–ª–∏ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ—Ç (–∏–∑-–∑–∞ –æ—á–∏—Å—Ç–∫–∏), –ø—Ä–æ–±—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫—É:
            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 70% —Å—Ç—Ä–æ–∫–∏, –µ—Å–ª–∏ –æ–Ω–∞ –¥–ª–∏–Ω–Ω–∞—è
            pass 

        if candidate_same_line:
            # –ï—Å–ª–∏ –æ–∂–∏–¥–∞–µ–º —á–∏—Å–ª–æ
            if expected_type == "number":
                num = self._extract_number(candidate_same_line)
                if num: return True, num, target_idx
            # –ï—Å–ª–∏ –æ–∂–∏–¥–∞–µ–º —Å—Ç—Ä–æ–∫—É/–¥–∞—Ç—É
            else:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞: –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ—Å—Ç–∞—Ç–æ–∫ –ø—Ä–æ—Å—Ç–æ –º—É—Å–æ—Ä–æ–º –∏–ª–∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–æ–º
                if len(candidate_same_line) > 1 and not any(sw in candidate_same_line.lower() for sw in self.stop_words):
                    return True, candidate_same_line, target_idx

        # === –°–¢–†–ê–¢–ï–ì–ò–Ø 1: –ü–û–ò–°–ö –í –°–õ–ï–î–£–Æ–©–ò–• –°–¢–†–û–ö–ê–• ===
        # –ï—Å–ª–∏ –Ω–∞ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–æ–∫–µ –ø—É—Å—Ç–æ, —Å–º–æ—Ç—Ä–∏–º –≤–Ω–∏–∑ (–º–∞–∫—Å 4 —Å—Ç—Ä–æ–∫–∏)
        for k in range(target_idx + 1, min(len(doc_paragraphs), target_idx + 5)):
            cand = doc_paragraphs[k].strip()
            if not cand: continue
            
            # –°—Ç–æ–ø-—Ñ–∞–∫—Ç–æ—Ä—ã
            cand_lower = cand.lower()
            if any(sw in cand_lower for sw in self.stop_words): break
            if re.match(r"^\d+\.", cand): break # –°–ª–µ–¥—É—é—â–∏–π –ø—É–Ω–∫—Ç —Å–ø–∏—Å–∫–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 4. –ó–∞–∫–ª—é—á–µ–Ω–∏–µ)

            if expected_type == "number":
                num = self._extract_number(cand)
                if num: return True, num, k
            else:
                return True, cand, k

        return False, None, target_idx

    def _extract_number(self, text):
        # –ò—â–µ—Ç —á–∏—Å–ª–æ, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –æ–∫—Ä—É–∂–∞—é—â–∏–π —Ç–µ–∫—Å—Ç
        m = re.search(r"([+-]?\s*\d+([.,]\d+)?)", text)
        if m: return m.group(1).replace(" ", "")
        return None

# ============================================================
#  AGENT 4: ANALYST AGENT (–ê–≥–µ–Ω—Ç-–ê–Ω–∞–ª–∏—Ç–∏–∫)
#  –†–æ–ª—å: –í–∞–ª–∏–¥–∞—Ü–∏—è —Ç–∏–ø–æ–≤, –ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑, –õ–æ–≥–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã.
# ============================================================

class AnalystAgent:
    def validate_type(self, value: str, expected_type: str) -> bool:
        if not value: return False
        v = value.strip()
        if expected_type == "string": 
            return bool(re.search(r"[A-Za-z–ê-–Ø–∞-—è–Å—ë]", v))
        if expected_type == "number": 
            # –°—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–∏—Å–ª–∞
            clean_v = v.replace(' ', '').replace(',', '.')
            try:
                float(clean_v)
                return True
            except ValueError:
                return False
        if expected_type == "date":
            return bool(re.search(r"\d{1,2}[\.\s][\w\.]+\s?\d{4}", v))
        return True

    def analyze_groups(self, extraction_results: List[Dict]) -> List[Dict]:
        """
        –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö, –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—É–º–º/—Å—Ä–µ–¥–Ω–∏—Ö
        –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ª–æ–≤–∏–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, SUM=100).
        """
        groups = {}
        # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
        for res in extraction_results:
            g_name = res.get("group_name")
            g_cond = res.get("group_condition")
            if not g_name or not g_cond: continue
            
            key = (g_name, g_cond)
            if key not in groups:
                groups[key] = {"values": [], "missing": []}
            
            if res["status"] == "ok" and res["expected_type"] == "number":
                try:
                    val = float(res["value"].replace(',', '.').replace(' ', ''))
                    groups[key]["values"].append(val)
                except:
                    groups[key]["missing"].append(res["field"])
            else:
                groups[key]["missing"].append(res["field"])

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—ã–≤–æ–¥–æ–≤
        analysis_report = []
        for (name, condition), data in groups.items():
            if data["missing"]:
                analysis_report.append({
                    "type": "group_error",
                    "msg": f"‚ö†Ô∏è –ì—Ä—É–ø–ø–∞ '{name}': –ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —É—Å–ª–æ–≤–∏–µ '{condition}'. –û—à–∏–±–∫–∏ –≤ –ø–æ–ª—è—Ö: {', '.join(data['missing'])}"
                })
                continue

            # –ü–∞—Ä—Å–∏–Ω–≥ —É—Å–ª–æ–≤–∏—è: (SUM|AVG)([<=>!]+)(\d+)
            m = re.match(r"(SUM|AVG)([<=>!]+)(\d+(\.\d+)?)", condition.upper().replace(' ', ''))
            if not m:
                analysis_report.append({"type": "group_error", "msg": f"‚ùå –ì—Ä—É–ø–ø–∞ '{name}': –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å —É—Å–ª–æ–≤–∏—è '{condition}'"})
                continue
            
            op_type, operator, target_str = m.group(1), m.group(2), m.group(3)
            target = float(target_str)
            
            # –í—ã—á–∏—Å–ª–µ–Ω–∏—è
            calculated = sum(data["values"])
            if op_type == "AVG" and data["values"]:
                calculated /= len(data["values"])
            
            # –õ–æ–≥–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
            valid = False
            if operator == '=': valid = abs(calculated - target) < 0.01
            elif operator == '>': valid = calculated > target
            elif operator == '<': valid = calculated < target
            elif operator == '>=': valid = calculated >= target
            elif operator == '<=': valid = calculated <= target
            
            icon = "‚úÖ" if valid else "‚ùå"
            result_text = "—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç" if valid else "–ù–ï —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç"
            
            analysis_report.append({
                "type": "group_result",
                "valid": valid,
                "msg": f"{icon} –ì—Ä—É–ø–ø–∞ '{name}': {op_type} = {calculated:.2f}. –≠—Ç–æ {result_text} —É—Å–ª–æ–≤–∏—é {condition}."
            })
            
        return analysis_report

# ============================================================
#  SYSTEM: COORDINATOR (–û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä)
# ============================================================

class MultiAgentCheckSystem:
    def __init__(self):
        self.perceptor = PerceptionAgent()
        self.schema = SchemaAgent()
        self.extractor = ExtractionAgent()
        self.analyst = AnalystAgent()

    def process(self, template_path: str, doc_path: str) -> str:
        try:
            # 1. –í–æ—Å–ø—Ä–∏—è—Ç–∏–µ
            tpl_paras = self.perceptor.load_content(template_path)
            doc_paras = self.perceptor.load_content(doc_path)

            # 2. –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å—Ö–µ–º—ã
            plan = self.schema.parse_template(tpl_paras)
            if not plan:
                return "‚ùå –û—à–∏–±–∫–∞: –í —à–∞–±–ª–æ–Ω–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–µ–≥–æ–≤ –≤–∏–¥–∞ [[name:type]]."

            # 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏ –ø–µ—Ä–≤–∏—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
            results = []
            cursor = 0
            # –ü—Ä–æ–ø—É—Å–∫ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫ –≤ –Ω–∞—á–∞–ª–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
            while cursor < len(doc_paras) and not doc_paras[cursor].strip():
                cursor += 1
            
            for item in plan:
                found, val, idx = self.extractor.find_value(item, doc_paras, cursor)
                
                res = {
                    "field": item["name"],
                    "expected_type": item["type"],
                    "group_name": item["group_name"],
                    "group_condition": item["group_condition"],
                    "value": val,
                    "optional": item["optional"]
                }
                
                if found:
                    is_valid = self.analyst.validate_type(val, item["type"])
                    res["status"] = "ok" if is_valid else "type_error"
                    # –°–¥–≤–∏–≥–∞–µ–º –∫—É—Ä—Å–æ—Ä, –Ω–æ –Ω–µ —Å–ª–∏—à–∫–æ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ, –µ—Å–ª–∏ —ç—Ç–æ —Ç–∞–±–ª–∏—Ü–∞
                    cursor = max(cursor, idx) 
                else:
                    res["status"] = "missing_optional" if item["optional"] else "missing"
                
                results.append(res)

            # 4. –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–ì—Ä—É–ø–ø—ã –∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞)
            group_analysis = self.analyst.analyze_groups(results)

            # 5. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
            return self._generate_human_report(doc_path, results, group_analysis)

        except Exception as e:
            logging.error(f"System Error: {e}", exc_info=True)
            return f"üî• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å–∏—Å—Ç–µ–º—ã: {str(e)}"

    def _generate_human_report(self, doc_name, results, group_analysis):
        lines = [f"ü§ñ <b>–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏</b>", f"üìÑ –§–∞–π–ª: {os.path.basename(doc_name)}", ""]
        
        lines.append("<b>1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–µ–π:</b>")
        for r in results:
            if r["status"] == "ok":
                lines.append(f"‚úÖ <b>{r['field']}</b>: {r['value']}")
            elif r["status"] == "type_error":
                lines.append(f"‚ö†Ô∏è <b>{r['field']}</b>: '{r['value']}' (–ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø, –∂–¥—É {r['expected_type']})")
            elif r["status"] == "missing":
                lines.append(f"‚ùå <b>{r['field']}</b>: –ù–µ –Ω–∞–π–¥–µ–Ω–æ")
            elif r["status"] == "missing_optional":
                lines.append(f"‚ÑπÔ∏è {r['field']}: –ø—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ–æ–±—è–∑.)")

        if group_analysis:
            lines.append("\n<b>2. –õ–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑:</b>")
            for ga in group_analysis:
                lines.append(ga["msg"])
        
        return "\n".join(lines)

# ============================================================
#  TELEGRAM BOT LOGIC
# ============================================================

BOT_TOKEN = "8124707173:AAEUWIG6cU8ErdX_ItQZdbWNGD3JRLwjjNo" # <-- –í—Å—Ç–∞–≤—å—Ç–µ —Ç–æ–∫–µ–Ω

logging.basicConfig(level=logging.INFO)
bot = Bot(token=BOT_TOKEN)
dp = Dispatcher(storage=MemoryStorage())
system = MultiAgentCheckSystem()

TEMP_DIR = "temp_files"
os.makedirs(TEMP_DIR, exist_ok=True)

class Workflow(StatesGroup):
    waiting_for_template = State()
    waiting_for_document = State()

@dp.message(Command("start"))
async def cmd_start(message: types.Message, state: FSMContext):
    await message.answer(
        "üëã –ü—Ä–∏–≤–µ—Ç! –Ø <b>–°–ê–ü–°–†</b>.\n\n"
        "–ü—Ä–∏—à–ª–∏—Ç–µ <b>–®–ê–ë–õ–û–ù</b> –≤ —Ñ–æ—Ä–º–∞—Ç–µ docx/pdf\n",
        parse_mode="HTML"
    )
    await state.set_state(Workflow.waiting_for_template)

@dp.message(Workflow.waiting_for_template, F.document)
async def process_template(message: types.Message, state: FSMContext):
    file_name = message.document.file_name
    if not (file_name.endswith('.docx') or file_name.endswith('.pdf')):
        await message.answer("‚ùå –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ .docx –∏ .pdf —Ñ–∞–π–ª—ã.")
        return

    file = await bot.get_file(message.document.file_id)
    local_path = os.path.join(TEMP_DIR, f"tpl_{message.from_user.id}_{file_name}")
    await bot.download_file(file.file_path, local_path)
    
    await state.update_data(template_path=local_path)
    await message.answer(f"‚úÖ –®–∞–±–ª–æ–Ω <b>{file_name}</b> –∑–∞–≥—Ä—É–∂–µ–Ω. \n–ñ–¥—É –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏.", parse_mode="HTML")
    await state.set_state(Workflow.waiting_for_document)

@dp.message(Workflow.waiting_for_document, F.document)
async def process_document(message: types.Message, state: FSMContext):
    data = await state.get_data()
    template_path = data.get("template_path")
    if not template_path:
        await message.answer("‚ö†Ô∏è –®–∞–±–ª–æ–Ω –ø–æ—Ç–µ—Ä—è–Ω. –ù–∞—á–Ω–∏—Ç–µ —Å /start")
        return

    msg = await message.answer("‚è≥ –ê–≥–µ–Ω—Ç—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –¥–∞–Ω–Ω—ã–µ...")
    
    file_name = message.document.file_name
    file = await bot.get_file(message.document.file_id)
    doc_path = os.path.join(TEMP_DIR, f"doc_{message.from_user.id}_{file_name}")
    await bot.download_file(file.file_path, doc_path)
    
    # –ó–∞–ø—É—Å–∫ —Å–∏—Å—Ç–µ–º—ã –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    report = await asyncio.to_thread(system.process, template_path, doc_path)
    
    # –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞—Å—Ç–∏, –µ—Å–ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ –¥–ª—è Telegram (4096 —Å–∏–º–≤–æ–ª–æ–≤)
    if len(report) > 4000:
        for x in range(0, len(report), 4000):
            await message.answer(report[x:x+4000], parse_mode="HTML")
    else:
        await msg.edit_text(report, parse_mode="HTML")
    
    await message.answer("–ú–æ–∂–µ—Ç–µ –ø—Ä–∏—Å–ª–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –∏–ª–∏ /start –¥–ª—è —Å–º–µ–Ω—ã —à–∞–±–ª–æ–Ω–∞.")

@dp.message(Command("cancel"))
async def cmd_cancel(message: types.Message, state: FSMContext):
    await state.clear()
    await message.answer("–°–±—Ä–æ—Å –≤—ã–ø–æ–ª–Ω–µ–Ω. –ñ–º–∏—Ç–µ /start")

async def main():
    await bot.delete_webhook(drop_pending_updates=True)
    await dp.start_polling(bot)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass
